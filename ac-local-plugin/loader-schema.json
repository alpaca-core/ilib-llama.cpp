{
  "id": "llama.cpp",
  "description": "Inference based on our fork of https://github.com/ggerganov/llama.cpp",
  "states": [
    {
      "id": "initial",
      "description": "Initial state",
      "ops": {
        "load-model": {
          "description": "Load the llama.cpp model",
          "params": {
            "type": "object",
            "properties": {
              "gguf": {
                "type": "string",
                "description": "Path to the file with model data."
              },
              "loras": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Paths to lora adapters.",
                "default": []
              }
            }
          },
          "return": {
            "type": "null"
          }
        }
      },
      "ins": null,
      "outs": null
    },
    {
      "id": "model-loaded",
      "description": "Model loaded state",
      "ops": {
        "start-instance": {
          "description": "Start a new instance of the llama.cpp model",
          "params": {
            "type": "object",
            "properties": {
              "instance_type": {
                "type": "string",
                "description": "Type of the instance to start",
                "default": "general"
              },
              "ctrl-vectors": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Paths to the control vectors.",
                "default": []
              },
              "ctx_size": {
                "type": "integer",
                "description": "Size of the context",
                "default": 0
              },
              "batch_size": {
                "type": "integer",
                "description": "Size of the single batch",
                "default": 2048
              },
              "ubatch_size": {
                "type": "integer",
                "description": "Size of the context",
                "default": 512
              }
            }
          },
          "return": {
            "type": "null"
          }
        }
      },
      "ins": null,
      "outs": null
    },
    {
      "id": "instance",
      "description": "Instance state",
      "ops": {
        "run": {
          "description": "Run the llama.cpp inference and produce some output",
          "params": {
            "type": "object",
            "properties": {
              "prompt": {
                "type": "string",
                "description": "Prompt to complete"
              },
              "antiprompts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Antiprompts to trigger stop",
                "default": []
              },
              "max_tokens": {
                "type": "integer",
                "description": "Maximum number of tokens to generate. 0 for unlimited",
                "default": 0
              }
            },
            "required": [
              "prompt"
            ]
          },
          "return": {
            "type": "object",
            "properties": {
              "result": {
                "type": "string",
                "description": "Generated result (completion of prompt)"
              }
            },
            "required": [
              "result"
            ]
          }
        },
        "begin-chat": {
          "description": "Begin a chat session",
          "params": {
            "type": "object",
            "properties": {
              "setup": {
                "type": "string",
                "description": "Initial setup for the chat session",
                "default": ""
              },
              "role_user": {
                "type": "string",
                "description": "Role name for the user",
                "default": "User"
              },
              "role_assistant": {
                "type": "string",
                "description": "Role name for the assistant",
                "default": "Assistant"
              }
            }
          },
          "return": {
            "type": "null"
          }
        }
      },
      "ins": null,
      "outs": null
    },
    {
      "id": "chat",
      "description": "Chat state",
      "ops": {
        "end-chat": {
          "description": "End a chat session",
          "params": {
            "type": "null"
          },
          "return": {
            "type": "null"
          }
        },
        "add-chat-prompt": {
          "description": "Add a prompt to the chat session as a user",
          "params": {
            "type": "object",
            "properties": {
              "prompt": {
                "type": "string",
                "description": "Prompt to add to the chat session",
                "default": ""
              }
            }
          },
          "return": {
            "type": "null"
          }
        },
        "get-chat-response": {
          "description": "Get a response from the chat session",
          "params": {
            "type": "null"
          },
          "return": {
            "type": "object",
            "properties": {
              "response": {
                "type": "string",
                "description": "Response from the chat session"
              }
            },
            "required": [
              "response"
            ]
          }
        }
      },
      "ins": null,
      "outs": null
    }
  ]
}
